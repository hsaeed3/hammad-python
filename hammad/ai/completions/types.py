"""hammad.ai.completions.types

Contains types for working with language model completions."""

import json
from typing import (
    Any,
    Dict,
    List,
    Generic,
    TypeVar,
    Optional,
    Union,
    Type,
    Iterator,
    AsyncIterator,
)

from pydantic import BaseModel, ConfigDict

try:
    from openai.types.chat import (
        ChatCompletionMessageParam,
        ChatCompletionMessageToolCall,
    )
except ImportError:
    raise ImportError(
        "Using the `hammad.ai.completions` extension requires the `openai` package to be installed.\n"
        "Please either install the `openai` package, or install the `hammad.ai` extension with:\n"
        "`pip install 'hammad-python[ai]'"
    )


__all__ = (
    "Completion",
    "CompletionsInputParam",
    "CompletionsOutputType",
    "CompletionChunk",
    "CompletionStream",
    "AsyncCompletionStream",
)


CompletionsInputParam = Union[
    str, ChatCompletionMessageParam, List[ChatCompletionMessageParam], Any
]
"""Type alias for the input parameters of a completion."""


CompletionsOutputType = TypeVar("CompletionsOutputType")
"""Type variable for the output type of a completion."""


class Completion(BaseModel, Generic[CompletionsOutputType]):
    """Extended response object for completions and structured outputs
    generated by language models using the `completions` resource
    within the `hammad.ai` extension."""

    model_config = ConfigDict(arbitrary_types_allowed=True)

    output: CompletionsOutputType
    """The output content of the completion. This is in the type that was
    requested within the `type` parameter."""

    model: str
    """The model that was used to generate the completion."""

    content: str | None = None
    """The actual response content of the completion. This is the string that
    was generated by the model."""

    tool_calls: List[ChatCompletionMessageToolCall] | None = None
    """The tool calls that were made by the model. This is a list of tool calls
    that were made by the model."""

    refusal: str | None = None
    """The refusal message generated by the model. This is the string that
    was generated by the model when it refused to generate the completion."""

    completion: Any | None = None
    """The original completion object in the OpenAI Chat Compeltions specification,
    generated by the model."""

    def has_tool_calls(self, tools: str | List[str] | None = None) -> bool:
        """Checks whether the completion has tool calls in general,
        or if the tool calls are for a specific tool.

        Args:
            tools : The tool(s) to check for. If None, checks for any tool calls.

        Returns:
            bool : True if the completion has tool calls, False otherwise.
        """
        if self.tool_calls is None:
            return False
        if tools is None and self.tool_calls is not None:
            return True

        if tools:
            if not isinstance(tools, list):
                tools = [tools]
            return any(
                tool_call.function.name in tools for tool_call in self.tool_calls
            )
        return False

    def get_tool_call_parameters(
        self, tool: str | None = None
    ) -> Dict[str, Any] | None:
        """Returns the generated parameters for a tool
        call within a completion. If the completion has multiple tool calls,
        and no tool is specified, an error will be raised.

        Args:
            tool : The name of the tool to get the parameters for.

        Returns:
            Dict[str, Any] : The generated parameters for the tool call.
        """
        if self.tool_calls is None:
            return None

        if tool is None:
            if len(self.tool_calls) > 1:
                raise ValueError(
                    "Multiple tool calls found in completion, and no tool specified."
                )
            tool = self.tool_calls[0].function.name

        for tool_call in self.tool_calls:
            if tool_call.function.name == tool:
                return json.loads(tool_call.function.arguments)
        return None

    def to_message(self) -> ChatCompletionMessageParam:
        """Convert the completion to a ChatCompletionMessageParam.

        This method converts the completion into a message that can be used
        in subsequent chat completion calls. It handles different output types
        appropriately.

        Returns:
            ChatCompletionMessageParam: The completion as a chat message
        """
        if self.tool_calls:
            # If there are tool calls, return assistant message with tool calls
            return {
                "role": "assistant",
                "content": self.content,
                "tool_calls": [
                    {
                        "id": tool_call.id,
                        "type": "function",
                        "function": {
                            "name": tool_call.function.name,
                            "arguments": tool_call.function.arguments,
                        },
                    }
                    for tool_call in self.tool_calls
                ],
            }
        elif self.refusal:
            # If there's a refusal, return assistant message with refusal
            return {"role": "assistant", "refusal": self.refusal}
        else:
            # Standard assistant response
            content = self.content
            if content is None and self.output != self.content:
                # For structured outputs, convert to string if needed
                if hasattr(self.output, "model_dump_json"):
                    content = self.output.model_dump_json()
                elif hasattr(self.output, "__dict__"):
                    content = json.dumps(self.output.__dict__)
                else:
                    content = str(self.output)

            return {"role": "assistant", "content": content or str(self.output)}


class CompletionChunk(BaseModel, Generic[CompletionsOutputType]):
    """Represents a chunk of data from a completion stream.

    This class unifies chunks from both LiteLLM and Instructor streaming,
    providing a consistent interface for processing streaming completions.
    """

    model_config = ConfigDict(arbitrary_types_allowed=True)

    content: str | None = None
    """The content delta for this chunk."""

    output: CompletionsOutputType | None = None
    """The structured output for this chunk (from instructor)."""

    model: str | None = None
    """The model that generated this chunk."""

    finish_reason: str | None = None
    """The reason the stream finished (if applicable)."""

    chunk: Any | None = None
    """The original chunk object from the provider."""

    is_final: bool = False
    """Whether this is the final chunk in the stream."""

    def __bool__(self) -> bool:
        """Check if this chunk has meaningful content."""
        return bool(self.content or self.output or self.finish_reason)


class CompletionStream(Generic[CompletionsOutputType]):
    """Synchronous stream wrapper for completion streaming.

    This class provides a unified interface for streaming completions
    from both LiteLLM and Instructor, handling the different chunk
    formats and providing consistent access patterns.
    """

    def __init__(
        self,
        stream: Iterator[Any],
        output_type: Type[CompletionsOutputType] = str,
        model: str | None = None,
    ):
        self._stream = stream
        self._output_type = output_type
        self._model = model
        self._chunks: List[CompletionChunk] = []
        self._final_output: CompletionsOutputType | None = None
        self._is_instructor = output_type != str
        self._is_consumed = False

    def __iter__(self) -> Iterator[CompletionChunk]:
        """Iterate over completion chunks."""
        for chunk in self._stream:
            completion_chunk = self._process_chunk(chunk)
            if completion_chunk:
                self._chunks.append(completion_chunk)
                yield completion_chunk
        self._is_consumed = True

    def _process_chunk(self, chunk: Any) -> CompletionChunk | None:
        """Process a raw chunk into a CompletionChunk."""
        if self._is_instructor:
            # Handle instructor streaming (Partial/Iterable)
            # Extract .value if it exists (for converted non-Pydantic types)
            output = chunk
            if hasattr(chunk, "value"):
                output = chunk.value

            return CompletionChunk(
                output=output,
                model=self._model,
                chunk=chunk,
                is_final=hasattr(chunk, "_is_final") and chunk._is_final,
            )
        else:
            # Handle LiteLLM streaming (ChatCompletionChunk)
            if hasattr(chunk, "choices") and chunk.choices:
                choice = chunk.choices[0]
                content = None
                if hasattr(choice, "delta") and choice.delta:
                    content = getattr(choice.delta, "content", None)

                return CompletionChunk(
                    content=content,
                    model=getattr(chunk, "model", self._model),
                    finish_reason=getattr(choice, "finish_reason", None),
                    chunk=chunk,
                    is_final=getattr(choice, "finish_reason", None) is not None,
                )
        return None

    def collect(self) -> Completion[CompletionsOutputType]:
        """Collect all chunks and return a complete Completion object."""
        if not self._chunks:
            # Consume the stream if not already consumed
            list(self)

        if self._is_instructor and self._chunks:
            # For instructor, the final chunk contains the complete object
            # The output is already extracted (.value) in _process_chunk if needed
            final_chunk = self._chunks[-1]
            return Completion(
                output=final_chunk.output,
                model=final_chunk.model or self._model or "unknown",
                content=None,
                completion=None,
            )
        else:
            # For LiteLLM, combine content from all chunks
            content_parts = [chunk.content for chunk in self._chunks if chunk.content]
            combined_content = "".join(content_parts)

            return Completion(
                output=combined_content,
                model=self._model or "unknown",
                content=combined_content,
                completion=None,  # Don't set mock chunks as completion
            )

    def to_completion(self) -> Completion[CompletionsOutputType]:
        """Convert the stream to a Completion object.

        This method can only be called after the stream has been fully consumed.
        It's an alias for collect() with a check for consumption state.

        Returns:
            Completion[CompletionsOutputType]: The complete completion object

        Raises:
            RuntimeError: If the stream has not been fully consumed
        """
        if not self._is_consumed and not self._chunks:
            raise RuntimeError(
                "Stream must be fully consumed before converting to completion. Use collect() or iterate through the stream first."
            )

        return self.collect()

    def to_message(self) -> ChatCompletionMessageParam:
        """Convert the stream to a ChatCompletionMessageParam.

        This method can only be called after the stream has been fully consumed.
        It converts the final completion to a message format.

        Returns:
            ChatCompletionMessageParam: The completion as a chat message

        Raises:
            RuntimeError: If the stream has not been fully consumed
        """
        if not self._is_consumed and not self._chunks:
            raise RuntimeError(
                "Stream must be fully consumed before converting to message. Use collect() or iterate through the stream first."
            )

        completion = self.collect()
        return completion.to_message()


class AsyncCompletionStream(Generic[CompletionsOutputType]):
    """Asynchronous stream wrapper for completion streaming.

    This class provides a unified interface for async streaming completions
    from both LiteLLM and Instructor, handling the different chunk
    formats and providing consistent access patterns.
    """

    def __init__(
        self,
        stream: AsyncIterator[Any],
        output_type: Type[CompletionsOutputType] = str,
        model: str | None = None,
    ):
        self._stream = stream
        self._output_type = output_type
        self._model = model
        self._chunks: List[CompletionChunk] = []
        self._final_output: CompletionsOutputType | None = None
        self._is_instructor = output_type != str
        self._is_consumed = False

    def __aiter__(self) -> AsyncIterator[CompletionChunk]:
        """Async iterate over completion chunks."""
        return self

    async def __anext__(self) -> CompletionChunk:
        """Get the next completion chunk."""
        try:
            chunk = await self._stream.__anext__()
            completion_chunk = self._process_chunk(chunk)
            if completion_chunk:
                self._chunks.append(completion_chunk)
                return completion_chunk
            else:
                return await self.__anext__()  # Skip empty chunks
        except StopAsyncIteration:
            self._is_consumed = True
            raise StopAsyncIteration

    def _process_chunk(self, chunk: Any) -> CompletionChunk | None:
        """Process a raw chunk into a CompletionChunk."""
        if self._is_instructor:
            # Handle instructor streaming (Partial/Iterable)
            # Extract .value if it exists (for converted non-Pydantic types)
            output = chunk
            if hasattr(chunk, "value"):
                output = chunk.value

            return CompletionChunk(
                output=output,
                model=self._model,
                chunk=chunk,
                is_final=hasattr(chunk, "_is_final") and chunk._is_final,
            )
        else:
            # Handle LiteLLM streaming (ChatCompletionChunk)
            if hasattr(chunk, "choices") and chunk.choices:
                choice = chunk.choices[0]
                content = None
                if hasattr(choice, "delta") and choice.delta:
                    content = getattr(choice.delta, "content", None)

                return CompletionChunk(
                    content=content,
                    model=getattr(chunk, "model", self._model),
                    finish_reason=getattr(choice, "finish_reason", None),
                    chunk=chunk,
                    is_final=getattr(choice, "finish_reason", None) is not None,
                )
        return None

    async def collect(self) -> Completion[CompletionsOutputType]:
        """Collect all chunks and return a complete Completion object."""
        if not self._chunks:
            # Consume the stream if not already consumed
            async for _ in self:
                pass

        if self._is_instructor and self._chunks:
            # For instructor, the final chunk contains the complete object
            # The output is already extracted (.value) in _process_chunk if needed
            final_chunk = self._chunks[-1]
            return Completion(
                output=final_chunk.output,
                model=final_chunk.model or self._model or "unknown",
                content=None,
                completion=None,
            )
        else:
            # For LiteLLM, combine content from all chunks
            content_parts = [chunk.content for chunk in self._chunks if chunk.content]
            combined_content = "".join(content_parts)

            return Completion(
                output=combined_content,
                model=self._model or "unknown",
                content=combined_content,
                completion=None,  # Don't set mock chunks as completion
            )

    async def to_completion(self) -> Completion[CompletionsOutputType]:
        """Convert the stream to a Completion object.

        This method can only be called after the stream has been fully consumed.
        It's an alias for collect() with a check for consumption state.

        Returns:
            Completion[CompletionsOutputType]: The complete completion object

        Raises:
            RuntimeError: If the stream has not been fully consumed
        """
        if not self._is_consumed and not self._chunks:
            raise RuntimeError(
                "Stream must be fully consumed before converting to completion. Use collect() or iterate through the stream first."
            )

        return await self.collect()

    async def to_message(self) -> ChatCompletionMessageParam:
        """Convert the stream to a ChatCompletionMessageParam.

        This method can only be called after the stream has been fully consumed.
        It converts the final completion to a message format.

        Returns:
            ChatCompletionMessageParam: The completion as a chat message

        Raises:
            RuntimeError: If the stream has not been fully consumed
        """
        if not self._is_consumed and not self._chunks:
            raise RuntimeError(
                "Stream must be fully consumed before converting to message. Use collect() or iterate through the stream first."
            )

        completion = await self.collect()
        return completion.to_message()
